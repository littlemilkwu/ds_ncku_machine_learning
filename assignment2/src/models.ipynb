{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "RANDOM_STATE = 11\n",
    "pd.set_option('display.max_columns', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv(\"../output/train_X.csv\")\n",
    "train_y = pd.read_csv(\"../output/train_y.csv\").squeeze()\n",
    "\n",
    "test_X = pd.read_csv(\"../output/test_X.csv\")\n",
    "test_y = pd.read_csv(\"../output/test_y.csv\").squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X shape   : (37499, 55)\n",
      "train_y shape   : (37499,)\n",
      "--------------------\n",
      "val_X shape     : (9375, 55)\n",
      "val_y shape     : (9375,)\n",
      "--------------------\n",
      "test_X shape    : (11718, 55)\n",
      "test_y shape    : (11718,)\n"
     ]
    }
   ],
   "source": [
    "train_X, val_X, train_y, val_y = train_test_split(train_X, train_y, train_size=0.8, random_state=RANDOM_STATE)\n",
    "print('{:<15} :'.format('train_X shape'), train_X.shape)\n",
    "print('{:<15} :'.format('train_y shape'), train_y.shape)\n",
    "print('-'*20)\n",
    "print('{:<15} :'.format('val_X shape'), val_X.shape)\n",
    "print('{:<15} :'.format('val_y shape'), val_y.shape)\n",
    "print('-'*20)\n",
    "print('{:<15} :'.format('test_X shape'), test_X.shape)\n",
    "print('{:<15} :'.format('test_y shape'), test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_cols = [\"airbags\", \"is_esc\", \"is_adjustable_steering\", \"is_tpms\",\n",
    "#             \"is_parking_sensors\", \"is_parking_camera\", \"is_front_fog_lights\",\n",
    "#             \"is_rear_window_wiper\", \"is_rear_window_washer\", \"is_rear_window_defogger\", \"is_brake_assist\", \"\"]\n",
    "# num_cols = [\"policy_tenure\", \"age_of_car\", \"age_of_policyholder\", \"area_cluster\",\n",
    "#             \"population_density\", \"model\", \"max_torque\", \"max_power\",\n",
    "#             \"engine_type\", \"displacement\", \"gear_box\", \"displacement\",\n",
    "#             \"turning_radius\", \"length\", \"width\", \"height\",\n",
    "#             \"gross_weight\", ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "<pre>\n",
    "若是類別資料，計算兩種狀況各自的條件機率\n",
    "若是數值資料，透過 guassian MLE 來計算可能性\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy        : 0.46944871138419525\n",
      "Precision       : 0.06671936758893281\n",
      "Recall          : 0.5733695652173914\n",
      "F1              : 0.11952981164141055\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.46      0.62     10982\n",
      "           1       0.07      0.57      0.12       736\n",
      "\n",
      "    accuracy                           0.47     11718\n",
      "   macro avg       0.50      0.52      0.37     11718\n",
      "weighted avg       0.89      0.47      0.59     11718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def _divide_cat_num_cols(self):\n",
    "        num_cols = []\n",
    "        cat_cols = []\n",
    "        for col in self.X.columns:\n",
    "            if len(self.X[col].unique()) > 5:\n",
    "                num_cols.append(col)\n",
    "            else:\n",
    "                cat_cols.append(col)\n",
    "        return num_cols, cat_cols\n",
    "\n",
    "    def _likelihood(self, x, mean, var):\n",
    "        eps = 1e-4\n",
    "        # print((1 / np.sqrt(2 * np.pi * var + eps)) * np.exp( -1 * (x-mean)**2 / (2 * var + eps)))\n",
    "        return (1 / np.sqrt(2 * np.pi * var + eps)) * np.exp( -1 * (x-mean)**2 / (2 * var + eps))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.num_cols, self.cat_cols = self._divide_cat_num_cols()\n",
    "        self.y_classes = y.unique()\n",
    "        self.parameter = dict()\n",
    "\n",
    "        for c in self.y_classes:\n",
    "            mask = (y == c)\n",
    "            c_X = X[mask]\n",
    "            self.parameter[c] = dict()\n",
    "            for col in self.X.columns:\n",
    "                if col in self.num_cols:\n",
    "                    # 數值特徵紀錄 mean, var\n",
    "                    self.parameter[c][col] = [\n",
    "                        round(c_X[col].mean(), 2),\n",
    "                        round(c_X[col].var(), 2)\n",
    "                    ]\n",
    "                elif col in self.cat_cols:\n",
    "                    # 類別特徵直接計算各類別的機率\n",
    "                    self.parameter[c][col] = dict()\n",
    "                    for cat in c_X[col].unique():\n",
    "                        mask = (c_X[col] == cat)\n",
    "                        self.parameter[c][col][cat] = round(len(c_X.loc[mask] ) / len(c_X), 2)\n",
    "        return self.parameter\n",
    "    def _calc_prob(self, X):\n",
    "        all_prob = []\n",
    "        for c in self.y_classes:\n",
    "            c_prob = 1\n",
    "            for key, value in X.items():\n",
    "                if key in self.num_cols:\n",
    "                    mean, var = self.parameter[c][key]\n",
    "                    c_prob *= self._likelihood(value, mean, var)\n",
    "                elif key in self.cat_cols:\n",
    "                    c_prob *= self.parameter[c][key][value]\n",
    "            all_prob.append(c_prob)\n",
    "        return all_prob\n",
    "\n",
    "    def predict(self, test_X):\n",
    "        pred_y = np.array([self._calc_prob(X) for i, X in test_X.iterrows()])\n",
    "        return pred_y.argmax(axis=1)\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    \n",
    "\n",
    "nb_clf = NaiveBayesClassifier()\n",
    "nb_clf.fit(train_X, train_y)\n",
    "pred_y = nb_clf.predict(test_X)\n",
    "evaluate(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "        特徵：data, 剩餘 features, left_child, right_child\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def _calc_gini(y):\n",
    "        total_len = len(y)\n",
    "        if total_len == 0:\n",
    "            return 0\n",
    "        pos_len = len(y[y==1])\n",
    "        neg_len = total_len - pos_len\n",
    "        return 1 - (pos_len / total_len)**2 - (neg_len / total_len)**2\n",
    "\n",
    "    def __init__(self, X, y, feat_candidate):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.feat_candidate = feat_candidate\n",
    "        self.gini = self._calc_gini(y)\n",
    "        self.l_child = None\n",
    "        self.r_child = None\n",
    "\n",
    "class DecisionTreeClassifier:\n",
    "    '''\n",
    "        * 二元類別：\n",
    "        * 多元類別：\n",
    "        * 數值特徵：sort -> 中位數切分\n",
    "    '''\n",
    "    @staticmethod\n",
    "    def _calc_gini(y):\n",
    "        total_len = len(y)\n",
    "        pos_len = len(y[y==1])\n",
    "        neg_len = total_len - pos_len\n",
    "        return 1 - (pos_len / total_len)**2 - (neg_len / total_len)**2\n",
    "\n",
    "    def __init__(self):\n",
    "        self.min_num = 15\n",
    "        pass\n",
    "\n",
    "    def choose_best_feat(self, node):\n",
    "        best_feat = node.feat_candidate[0]\n",
    "        best_gini_gain = 0\n",
    "        best_mask = None\n",
    "        feat_del = []\n",
    "        for feat in node.feat_candidate:\n",
    "            uniq = node.X[feat].unique()\n",
    "            if len(uniq) == 1:\n",
    "                # 類別，單獨值，此類別無用處\n",
    "                feat_del.append(feat)\n",
    "                continue\n",
    "            elif len(uniq) == 2:\n",
    "                # 類別\n",
    "                mask = (node.X[feat] == uniq[0])\n",
    "            else:\n",
    "                # 數值\n",
    "                median = np.median(node.X[feat].to_numpy())\n",
    "                mask = (node.X[feat] <= median)\n",
    "\n",
    "            # print('---choose feature')\n",
    "            gini_gain = (node.gini\n",
    "                - len(node.X[mask]) / len(node.X) * self._calc_gini(node.y[mask])\n",
    "                - len(node.X[~mask]) / len(node.X) * self._calc_gini(node.y[~mask]))\n",
    "            if gini_gain > best_gini_gain:\n",
    "                best_gini_gain = gini_gain\n",
    "                best_feat = feat\n",
    "                best_mask = mask\n",
    "        return best_feat, best_mask, feat_del\n",
    "\n",
    "    def divide_branch(self, node):\n",
    "        if node.gini == 0 or len(node.feat_candidate) == 0 or len(node.X) < self.min_num:\n",
    "            # 乾淨 node / 沒特徵可用 / 小於最小數量\n",
    "            return None\n",
    "        \n",
    "        best_feat, best_mask, feat_del = self.choose_best_feat(node)\n",
    "        node.feat_candidate = [feat for feat in node.feat_candidate if feat not in feat_del]\n",
    "\n",
    "        # print('--new branch')\n",
    "        \n",
    "        \n",
    "        if best_mask is not None:\n",
    "            # 左邊分支確認有找到\n",
    "            node.l_child = Node(\n",
    "                node.X[best_mask], node.y[best_mask],\n",
    "                feat_candidate=[feat for feat in node.feat_candidate if feat != best_feat])\n",
    "            # print('yes')\n",
    "            self.divide_branch(node.l_child)\n",
    "            \n",
    "\n",
    "        if len(node.y[~best_mask]) != 0:\n",
    "            # 右邊分支先確認數量\n",
    "            node.r_child = Node(\n",
    "                node.X[~best_mask], node.y[~best_mask],\n",
    "                feat_candidate=[feat for feat in node.feat_candidate if feat != best_feat])\n",
    "            self.divide_branch(node.r_child)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        self.root = Node(X, y, list(X.columns))\n",
    "        self.divide_branch(self.root)\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "# dt_clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifier:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        pass\n",
    "\n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    def predict(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier: sklearn version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy        : 0.5640040962621607\n",
      "Precision       : 0.09199477514461653\n",
      "Recall          : 0.6698369565217391\n",
      "F1              : 0.16177194421657096\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.56      0.71     10982\n",
      "           1       0.09      0.67      0.16       736\n",
      "\n",
      "    accuracy                           0.56     11718\n",
      "   macro avg       0.53      0.61      0.43     11718\n",
      "weighted avg       0.91      0.56      0.67     11718\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=100, max_depth=5,\n",
    "    class_weight='balanced' ,random_state=RANDOM_STATE)\n",
    "\n",
    "# rf_clf = RandomForestClassifier(\n",
    "#     n_estimators=100, max_depth=2, \n",
    "#     class_weight={0:1, 1:17} ,random_state=RANDOM_STATE)\n",
    "rf_clf.fit(train_X, train_y)\n",
    "pred_y = rf_clf.predict(test_X[train_X.columns])\n",
    "evaluate(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost, Catboost, LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "lgb_clf = LGBMClassifier(class_weight=\"balanced\")\n",
    "xgb_clf = XGBClassifier(scale_pos_weight=99)\n",
    "cat_clf = CatBoostClassifier(auto_class_weights='Balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lgb_clf.fit(train_X, train_y)\n",
    "# pred_y = lgb_clf.predict(test_X)\n",
    "# evaluate(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb_clf.fit(train_X, train_y)\n",
    "# pred_y = xgb_clf.predict(test_X[train_X.columns])\n",
    "# evaluate(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat_clf.fit(train_X, train_y, verbose=0)\n",
    "# pred_y = cat_clf.predict(test_X)\n",
    "# evaluate(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "<pre>\n",
    "k = 3, 5, 10\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolbox import cross_validation\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for k in [3, 5, 10]:\n",
    "    cv = cross_validation(train_X, train_y, k=k)\n",
    "    all_acc = dict()\n",
    "    for train_X, val_X, train_y, val_y, i in cv:\n",
    "        nb_clf = NaiveBayesClassifier()\n",
    "        rf_clf = RandomForestClassifier(\n",
    "                n_estimators=100, max_depth=5,\n",
    "                class_weight='balanced' ,random_state=RANDOM_STATE)\n",
    "        lgb_clf = LGBMClassifier(class_weight=\"balanced\")\n",
    "        xgb_clf = XGBClassifier(scale_pos_weight=99)\n",
    "        cat_clf = CatBoostClassifier(auto_class_weights='Balanced', verbose=0)\n",
    "        nb_clf.fit(train_X, train_y)\n",
    "        rf_clf.fit(train_X, train_y)\n",
    "        lgb_clf.fit(train_X, train_y)\n",
    "        xgb_clf.fit(train_X, train_y)\n",
    "        cat_clf.fit(train_X, train_y)\n",
    "\n",
    "        all_acc['nb'] = f1_score(val_y, nb_clf.predict(val_X))\n",
    "        all_acc['rf'] = f1_score(val_y, rf_clf.predict(val_X))\n",
    "        all_acc['lgb'] = f1_score(val_y, lgb_clf.predict(val_X))\n",
    "        all_acc['xgb'] = f1_score(val_y, xgb_clf.predict(val_X))\n",
    "        all_acc['cat'] = f1_score(val_y, cat_clf.predict(val_X))\n",
    "    result.append([\n",
    "        k, all_acc['nb'], all_acc['rf'], all_acc['lgb'], all_acc['xgb'], all_acc['cat']\n",
    "    ])\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>k</th>\n",
       "      <th>nb_f1</th>\n",
       "      <th>rf_f1</th>\n",
       "      <th>lgb_f1</th>\n",
       "      <th>xgb_f1</th>\n",
       "      <th>cat_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.128443</td>\n",
       "      <td>0.163924</td>\n",
       "      <td>0.168973</td>\n",
       "      <td>0.140397</td>\n",
       "      <td>0.152752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0.130027</td>\n",
       "      <td>0.165946</td>\n",
       "      <td>0.169072</td>\n",
       "      <td>0.151739</td>\n",
       "      <td>0.160593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.104712</td>\n",
       "      <td>0.141280</td>\n",
       "      <td>0.122667</td>\n",
       "      <td>0.114613</td>\n",
       "      <td>0.132450</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    k     nb_f1     rf_f1    lgb_f1    xgb_f1    cat_f1\n",
       "0   3  0.128443  0.163924  0.168973  0.140397  0.152752\n",
       "1   5  0.130027  0.165946  0.169072  0.151739  0.160593\n",
       "2  10  0.104712  0.141280  0.122667  0.114613  0.132450"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(result, columns=['k', 'nb_f1', 'rf_f1', 'lgb_f1', 'xgb_f1', 'cat_f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d43e22b5d3a7a0bd2e205ac27062b2ba3e10bf7033fb4f991038673b60a6900c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
